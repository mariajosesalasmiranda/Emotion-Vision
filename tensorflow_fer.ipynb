{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Emotion Recognition with TensorFlow and FER-2013 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we’ll explore face emotion recognition using [TensorFlow](https://en.wikipedia.org/wiki/TensorFlow), a powerful open-source library for building machine learning models.\n",
    "\n",
    "We’ll train a model to recognize different emotions, such as happiness, sadness, or anger, just by looking at facial expressions in images. This is possible because we’ll use [Convolutional Neural Networks (CNNs)](https://www.tensorflow.org/tutorials/images/cnn), a type of deep learning model that is great at recognizing patterns and features in images.\n",
    "\n",
    "CNNs automatically learn important details, like facial features, from the images, which allows the model to accurately predict emotions.\n",
    "\n",
    "<img src=\"https://scx2.b-cdn.net/gfx/news/hires/2019/threecnnmode.jpg\" alt=\"cnn\" width=\"550\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: build the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we define the architecture of our neural network using the ```Sequential``` model from Keras. This model will consist of several layers designed to extract features from input images and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1638528   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,713,927\n",
      "Trainable params: 1,713,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([ # 2D convolutional neural network\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)), # 48x48 image with 1 channel\n",
    "    MaxPooling2D(2, 2), # 2x2 pooling\n",
    "    Conv2D(128, (3, 3), activation='relu'), # 128 filters\n",
    "    MaxPooling2D(2, 2), # 2x2 pooling\n",
    "    Flatten(), # flatten to 1D\n",
    "    Dense(128, activation='relu'), # 128 neurons\n",
    "    Dropout(0.5), # 50% dropout\n",
    "    Dense(7, activation='softmax')  # 7 for 7 emotion classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # compile model\n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Conv2D Layers**: These convolutional layers (with 64 and 128 filters) are responsible for detecting features such as edges and textures in the images.\n",
    "- **MaxPooling2D Layers**: These layers reduce the spatial dimensions of the image, helping to decrease computation and focus on the most important features.\n",
    "- **Flatten Layer**: This layer converts the 2D features into a 1D vector, which can be processed by fully connected layers.\n",
    "- **Dense Layers**: These fully connected layers are where the final classification happens. The last dense layer outputs 7 values, each representing one of the 7 emotion classes (like happiness, sadness, etc.).\n",
    "- **Dropout Layer**: This layer helps prevent overfitting by randomly setting some neuron connections to zero during training.\n",
    "\n",
    "The model is compiled using the Adam optimizer and categorical crossentropy loss function, as it’s a multi-class classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: prepare the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we use the ```ImageDataGenerator``` from Keras to preprocess and augment the training and validation data. This helps the model generalize better and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 1432 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # normalize pixel values to [0,1] and split data into training and validation sets\n",
    "\n",
    "train_generator = datagen.flow_from_directory( # load training data\n",
    "    'fer_2013/train',  \n",
    "    target_size=(48, 48), # 48x48 images\n",
    "    color_mode='grayscale', # grayscale images\n",
    "    class_mode='categorical', # 7 classes\n",
    "    subset='training', # training set\n",
    "    batch_size=64 # 64 images per batch\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory( # load validation data\n",
    "    'fer_2013/test',\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Augmentation**: ```ImageDataGenerator``` rescales the pixel values by dividing them by 255 to normalize the image data.\n",
    "- **Training and Validation Split**: we set aside **20%** of the data for validation and use the rest for training.\n",
    "- **Flow from Directory**: ```flow_from_directory``` method loads the images from the specified directories (```'fer_2013/train'``` and ```'fer_2013/test'```) and resizes them to *48x48* pixels in grayscale mode, which matches the input shape of the model.\n",
    "\n",
    "These generators will help feed the images into the model during training and validation, allowing the model to learn and evaluate performance on different data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: train and save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we train the model using the fit method with the training and validation data generators we set up earlier. The model will learn to recognize facial emotions by processing the images over several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "359/359 [==============================] - 213s 590ms/step - loss: 1.7505 - accuracy: 0.2908 - val_loss: 1.5923 - val_accuracy: 0.3848\n",
      "Epoch 2/3\n",
      "359/359 [==============================] - 61s 169ms/step - loss: 1.5760 - accuracy: 0.3885 - val_loss: 1.4989 - val_accuracy: 0.4330\n",
      "Epoch 3/3\n",
      "359/359 [==============================] - 114s 318ms/step - loss: 1.4764 - accuracy: 0.4308 - val_loss: 1.3867 - val_accuracy: 0.4756\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( \n",
    "    train_generator, \n",
    "    validation_data=validation_generator, \n",
    "    epochs=3 \n",
    ")\n",
    "\n",
    "# save the model\n",
    "model.save('emotion_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training**: The model is trained for *3* epochs, meaning it will go through the entire training data three times. During each epoch, the model will adjust its weights to minimize the loss and improve accuracy.\n",
    "- **Validation**: The validation data is used to evaluate the model's performance at the end of each epoch, helping us monitor how well the model generalizes to unseen data.\n",
    "\n",
    "Finally, we save the trained model as an ```.h5``` file, so it can be reused or deployed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we use the trained model to recognize emotions from faces detected in real-time via a webcam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 317ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load pre-trained model and cascade classifier\n",
    "model = tf.keras.models.load_model('emotion_recognition_model.h5')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "        roi_gray = roi_gray / 255.0\n",
    "        roi_gray = np.expand_dims(roi_gray, axis=0)\n",
    "        roi_gray = np.expand_dims(roi_gray, axis=-1)\n",
    "        \n",
    "        # predict emotion\n",
    "        prediction = model.predict(roi_gray)\n",
    "        emotion = emotion_labels[np.argmax(prediction)]\n",
    "        \n",
    "        # draw rectangle and text\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load the Model**: the pre-trained emotion recognition model is loaded from the saved ```.h5``` file.\n",
    "- **Load Haar Cascade**: we also load the Haar Cascade classifier for detecting faces in the video feed.\n",
    "- **Emotion Labels**: a list of possible emotion labels (*Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral*) is defined for the output prediction.\n",
    "- **Video Capture**: we capture video from the webcam and continuously process the frames.\n",
    "    - **Face Detection**: each frame is converted to grayscale, and faces are detected using the Haar Cascade classifier.\n",
    "    - **Emotion Prediction**: the detected face regions are resized, normalized, and passed into the trained model for emotion prediction.\n",
    "    - **Draw Rectangle and Label**: a rectangle is drawn around the detected face, and the predicted emotion is displayed on the screen.\n",
    "\n",
    "This process continues in real-time until you press ```'q'``` to exit the webcam feed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
